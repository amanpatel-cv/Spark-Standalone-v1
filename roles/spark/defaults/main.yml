spark:
  version: 3.1.2
  hadoop_version: 3.2
  working_dir: /tmp/spark/data
  master_port: 7077
  worker_work_port: 65000
  master_ui_port: 8080
  worker_ui_port: 8085
  download_location: https://archive.apache.org/dist/spark/
  user: "spark"               # the name of the (OS)user created for spark
  user_groups: []             # Optional list of (OS)groups the new spark user should belong to
  user_shell: "/bin/false"    # the spark user's default shell

  env_extras: {}
  defaults_extras: {}
  defaults_extras_master: {"spark.eventLog.enabled":"true","spark.eventLog.dir":"file://{{ spark_history_dir }}","spark.history.fs.logDirectory":"file://{{ spark_history_dir }}","spark.eventLog.compress":"true","spark.history.fs.cleaner.enabled":"true","spark.history.fs.cleaner.interval":"1h","spark.history.fs.cleaner.maxAge":"4h","spark.history.fs.driverlog.cleaner.enabled":"true","spark.history.fs.driverlog.cleaner.interval":"1h", "spark.history.fs.driverlog.cleaner.maxAge":"4h"}

  defaults_extras_nodes: 
        - SPARK_WORKER_OPTS="-Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.appDataTtl=7200 -Dspark.worker.cleanup.interval=1800"
